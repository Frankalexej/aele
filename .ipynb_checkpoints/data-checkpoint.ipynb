{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e166c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0627b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = os.getcwd()\n",
    "work_dir = os.path.dirname(this_dir)\n",
    "data_dir = os.path.join(work_dir, 'data')\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e53012",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### setting area #####\n",
    "\n",
    "consonant = 'c' \n",
    "\n",
    "# mean values of the consonant\n",
    "\n",
    "cog = 7000\n",
    "\n",
    "fri_dur = 174\n",
    "\n",
    "# means of cog and frication duration\n",
    "\n",
    "'''\n",
    "ts: 4000, 96\n",
    "tc: 7000, 96\n",
    "s: 4000, 174\n",
    "c: 7000, 174\n",
    "\n",
    "Transcription for file names:\n",
    "\n",
    "tʂ: ts\n",
    "tɕ: tc\n",
    "ʂ: s\n",
    "ɕ: c\n",
    "\n",
    "'''\n",
    "\n",
    "vowel = 'i'\n",
    "\n",
    "# formants\n",
    "f_vals = [3372, 2761, 437] # f3, f2, f1\n",
    "\n",
    "'''\n",
    "i: 3372, 2761, 437\n",
    "ɪ: 3053, 2365, 483\n",
    "e: 3047, 2530, 536\n",
    "ɛ: 2979, 2058, 731\n",
    "u: 2735, 1105, 459\n",
    "ʊ: 2827, 1225, 519\n",
    "o: 2828, 1035, 555\n",
    "ɔ: 2824, 1136, 781\n",
    "\n",
    "Transcription for file names:\n",
    "\n",
    "i: i *\n",
    "ɪ: L\n",
    "e: e *\n",
    "ɛ: F\n",
    "u: u *\n",
    "ʊ: W\n",
    "o: o *\n",
    "ɔ: D\n",
    "\n",
    "*: used in training\n",
    "\n",
    "'''\n",
    "\n",
    "word = vowel + consonant + vowel\n",
    "\n",
    "# no. of tokens for each word\n",
    "sample_size = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98281c",
   "metadata": {},
   "source": [
    "### Consonant synthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6a9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd36b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_f_means = np.array([cog, fri_dur])\n",
    "c_f_stds = np.array([500, 13])\n",
    "\n",
    "con_means = np.array([200, 0.5, 1, 50, 60]) # sta_dev, skewness, kurtosis, bur_int, fri_int\n",
    "con_stds = con_means * 0.05\n",
    "\n",
    "con_means = np.concatenate((c_f_means, con_means))\n",
    "con_stds = np.concatenate((c_f_stds, con_stds))\n",
    "\n",
    "consonants = np.zeros((sample_size, len(con_means)))\n",
    "for i in range(len(con_means)):\n",
    "    a, b = - 2*con_stds[i] / con_stds[i], 2*con_stds[i] / con_stds[i]\n",
    "    dist = truncnorm(a, b, loc = con_means[i], scale = con_stds[i])\n",
    "    consonants[:, i] = dist.rvs(size = sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ed1be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.39202153e+03 1.59413894e+02 1.80659129e+02 5.12548945e-01\n",
      " 1.00966245e+00 5.17780215e+01 5.43419749e+01 2.00000000e+02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# total duration of fixed value 200 for consonants\n",
    "con_dur = np.full((sample_size, 1), 200)\n",
    "\n",
    "# zero values for all other features\n",
    "zeros = np.full((sample_size, 9), 0)\n",
    "\n",
    "# all concatenated\n",
    "consonants = np.hstack((consonants, con_dur, zeros))\n",
    "\n",
    "print(consonants[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7bc116",
   "metadata": {},
   "source": [
    "### Vowel synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62d2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "vow_means = np.array([80, 200, 170, 110 ,90]) # voc_int, f0, b3, b2, b1\n",
    "vow_means = np.append(vow_means, f_vals) # *, f3, f2, f1\n",
    "vow_stds = vow_means * 0.05\n",
    "\n",
    "vowels = np.zeros((sample_size, len(vow_means)))\n",
    "for i in range(len(vow_means)):\n",
    "    a, b = - 2*vow_stds[i] / vow_stds[i], 2*vow_stds[i] / vow_stds[i]\n",
    "    dist = truncnorm(a, b, loc = vow_means[i], scale = vow_stds[i])\n",
    "    vowels[:, i] = dist.rvs(size = sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422e3dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.            0.            0.            0.            0.\n",
      "    0.            0.          400.          400.           80.58371291\n",
      "  194.5556988   174.093556    114.74691289   93.93733349 3193.8479763\n",
      " 2855.30020509  415.27899351]\n"
     ]
    }
   ],
   "source": [
    "# vocalic duration and total duration of fixed value 400 for vowels\n",
    "vow_dur = np.full((sample_size, 2), 400)\n",
    "\n",
    "# zero values for all other features\n",
    "zeros = np.full((sample_size, 7), 0)\n",
    "\n",
    "# all concatenated\n",
    "vowels = np.hstack((zeros, vow_dur, vowels))\n",
    "\n",
    "print(vowels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e26edb",
   "metadata": {},
   "source": [
    "### A csv file as guideline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b857a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/Projects/phongen/data/structure_sample.csv\n"
     ]
    }
   ],
   "source": [
    "element = [\n",
    "    'cog', 'fri_dur', 'sta_dev', 'skewness', 'kurtosis', 'bur_int', 'fri_int', \n",
    "    'tot_dur', 'voc_dur', 'voc_int',\n",
    "    'f0', 'b3', 'b2', 'b1', 'f3', 'f2', 'f1'\n",
    "]\n",
    "\n",
    "explanation = [\n",
    "    'center of gravity', 'frication duration', 'standard deviation', 'skewness', 'kurtosis', 'burst intensity', 'frication intensity',\n",
    "    'total duration', 'vocalic duration', 'vocalic intensity',\n",
    "    'fundamental frequency', 'bandwidth of f3', 'bandwidth of f2', 'bandwidth of f1', 'f3', 'f2', 'f1'\n",
    "]\n",
    "\n",
    "mean = [\n",
    "    '4000, 7000', '96, 174', '200', '0.5', '1', '50', '60',\n",
    "    '200 for con, 400 for vow', '0 for con, 400 for vow', '0 for con, 80 for vow',\n",
    "    '200', '170', '110', '90', '', '', '']\n",
    "\n",
    "\n",
    "random = ['gaussian'] * 7 + ['fixed'] * 2 + ['gaussian'] * 8\n",
    "\n",
    "structure = pd.DataFrame({\n",
    "    'element': element,\n",
    "    'explanation': explanation, \n",
    "    'mean': mean,\n",
    "    'random': random,\n",
    "    'consonant_sample': consonants[0],\n",
    "    'consonant': consonant,\n",
    "    'vowel_sample': vowels[0],\n",
    "    'vowel': vowel\n",
    "})\n",
    "\n",
    "file_name = os.path.join(data_dir, 'structure_sample.csv')\n",
    "print(file_name)\n",
    "structure.to_csv(file_name, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167a6e7",
   "metadata": {},
   "source": [
    "### Save as 3*17 .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150c7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "\n",
    "subdata_dir = os.path.join(data_dir, word)\n",
    "os.makedirs(subdata_dir, exist_ok=True)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    uid = word + f'_{i+1:04d}'\n",
    "    filename = f'{uid}.npy'\n",
    "    save_path = os.path.join(subdata_dir, filename)\n",
    "    \n",
    "    vcv = np.vstack([vowels[i], consonants[i], vowels[i]])\n",
    "\n",
    "    # np.save(save_path, vcv)\n",
    "    \n",
    "    cog = vcv[1][0]\n",
    "    fri_dur = vcv[1][1]\n",
    "    \n",
    "    save_path_rel = os.path.relpath(save_path, start=work_dir)\n",
    "\n",
    "    metadata.append({\n",
    "        'uid': uid,\n",
    "        'path': save_path_rel,\n",
    "        'cog': cog,\n",
    "        'fri_dur': fri_dur,\n",
    "        'word': word\n",
    "    })\n",
    "\n",
    "metaframe = pd.DataFrame(metadata)\n",
    "\n",
    "csv_name = word + '_meta.csv'\n",
    "csv_path = os.path.join(data_dir, csv_name)\n",
    "# metaframe.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7e561",
   "metadata": {},
   "source": [
    "### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7733d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "this_dir = os.getcwd()\n",
    "work_dir = os.path.dirname(this_dir)\n",
    "data_dir = os.path.join(work_dir, 'data')\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "consonant_list = ['ts', 'tc', 's', 'c']\n",
    "consonant_cog = [4000, 7000, 4000, 7000]\n",
    "consonant_fd = [96, 96, 174, 174]\n",
    "\n",
    "vowel_list = ['i', 'L', 'e', 'F', 'u', 'W', 'o', 'D']\n",
    "vowel_formants = [\n",
    "    [3372, 2761, 437],\n",
    "    [3053, 2365, 483],\n",
    "    [3047, 2530, 536],\n",
    "    [2979, 2058, 731],\n",
    "    [2735, 1105, 459],\n",
    "    [2827, 1225, 519],\n",
    "    [2828, 1035, 555],\n",
    "    [2824, 1136, 781]\n",
    "]\n",
    "\n",
    "# no. of tokens for each word\n",
    "sample_size = 8000\n",
    "\n",
    "metadata = []\n",
    "\n",
    "for index in range (4):\n",
    "    for jndex in range (8):\n",
    "\n",
    "        consonant = consonant_list[index]\n",
    "        cog = consonant_cog[index]\n",
    "        fri_dur = consonant_fd[index]\n",
    "\n",
    "        vowel = vowel_list[jndex]\n",
    "        f_vals = vowel_formants[jndex]\n",
    "\n",
    "        word = vowel + consonant + vowel\n",
    "\n",
    "        if (index in [0, 3]) and (jndex in [0, 2, 4, 6]):\n",
    "            istrain = 'yes'\n",
    "        else:\n",
    "            istrain = 'no'\n",
    "\n",
    "        c_f_means = np.array([cog, fri_dur])\n",
    "        c_f_stds = np.array([500, 13])\n",
    "\n",
    "        con_means = np.array([200, 0.5, 1, 50, 60]) # sta_dev, skewness, kurtosis, bur_int, fri_int\n",
    "        con_stds = con_means * 0.05\n",
    "\n",
    "        con_means = np.concatenate((c_f_means, con_means))\n",
    "        con_stds = np.concatenate((c_f_stds, con_stds))\n",
    "\n",
    "        consonants = np.zeros((sample_size, len(con_means)))\n",
    "        for i in range(len(con_means)):\n",
    "            a, b = - 2*con_stds[i] / con_stds[i], 2*con_stds[i] / con_stds[i]\n",
    "            dist = truncnorm(a, b, loc = con_means[i], scale = con_stds[i])\n",
    "            consonants[:, i] = dist.rvs(size = sample_size)\n",
    "        \n",
    "        # total duration of fixed value 200 for consonants\n",
    "        con_dur = np.full((sample_size, 1), 200)\n",
    "\n",
    "        # zero values for all other features\n",
    "        zeros = np.full((sample_size, 9), 0)\n",
    "\n",
    "        # all concatenated\n",
    "        consonants = np.hstack((consonants, con_dur, zeros))\n",
    "\n",
    "        vow_means = np.array([80, 200, 170, 110 ,90]) # voc_int, f0, b3, b2, b1\n",
    "        vow_means = np.append(vow_means, f_vals) # *, f3, f2, f1\n",
    "        vow_stds = vow_means * 0.05\n",
    "\n",
    "        vowels = np.zeros((sample_size, len(vow_means)))\n",
    "        for i in range(len(vow_means)):\n",
    "            a, b = - 2*vow_stds[i] / vow_stds[i], 2*vow_stds[i] / vow_stds[i]\n",
    "            dist = truncnorm(a, b, loc = vow_means[i], scale = vow_stds[i])\n",
    "            vowels[:, i] = dist.rvs(size = sample_size)\n",
    "\n",
    "        # vocalic duration and total duration of fixed value 400 for vowels\n",
    "        vow_dur = np.full((sample_size, 2), 400)\n",
    "\n",
    "        # zero values for all other features\n",
    "        zeros = np.full((sample_size, 7), 0)\n",
    "\n",
    "        # all concatenated\n",
    "        vowels = np.hstack((zeros, vow_dur, vowels))\n",
    "\n",
    "        subdata_dir = os.path.join(data_dir, word)\n",
    "        os.makedirs(subdata_dir, exist_ok=True)\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            uid = word + f'_{i+1:04d}'\n",
    "            filename = f'{uid}.npy'\n",
    "            save_path = os.path.join(subdata_dir, filename)\n",
    "            \n",
    "            vcv = np.vstack([vowels[i], consonants[i], vowels[i]])\n",
    "\n",
    "            np.save(save_path, vcv)\n",
    "            \n",
    "            save_path_rel = os.path.relpath(save_path, start=work_dir)\n",
    "\n",
    "            metadata.append({\n",
    "                'uid': uid,\n",
    "                'path': save_path_rel,\n",
    "                'cog': vcv[1][0],\n",
    "                'fri_dur': vcv[1][1],\n",
    "                'word': word,\n",
    "                'consonant': consonant,\n",
    "                'vowel': vowel,\n",
    "                'train': istrain\n",
    "            })\n",
    "\n",
    "csv_name = 'metadata.csv'\n",
    "csv_path = os.path.join(data_dir, csv_name)\n",
    "metaframe = pd.DataFrame(metadata)\n",
    "metaframe.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd9b721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.00000000e+02\n",
      "  4.00000000e+02 8.29308996e+01 1.99760273e+02 1.79552239e+02\n",
      "  1.14265773e+02 9.15326853e+01 2.98373450e+03 1.16668291e+03\n",
      "  7.48867907e+02]\n",
      " [6.65778618e+03 1.70221202e+02 1.81112683e+02 4.60560255e-01\n",
      "  1.00853328e+00 4.72338549e+01 6.08201071e+01 2.00000000e+02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.00000000e+02\n",
      "  4.00000000e+02 8.29308996e+01 1.99760273e+02 1.79552239e+02\n",
      "  1.14265773e+02 9.15326853e+01 2.98373450e+03 1.16668291e+03\n",
      "  7.48867907e+02]]\n"
     ]
    }
   ],
   "source": [
    "savetest = np.load(save_path)\n",
    "\n",
    "print(savetest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c0ba2fc-cc99-4655-b1ae-dd01407b760f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.load(\"./../data/itsi/itsi_0001.npy\")\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977f2a9-5a91-4f45-8696-224511ea5642",
   "metadata": {},
   "source": [
    "## All Data (one file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b62401de-9761-473b-bd76-948f1453282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "this_dir = os.getcwd()\n",
    "work_dir = os.path.dirname(this_dir)\n",
    "data_dir = os.path.join(work_dir, 'data')\n",
    "\n",
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "consonant_list = ['ts', 'tc', 's', 'c']\n",
    "consonant_cog = [4000, 7000, 4000, 7000]\n",
    "consonant_fd = [96, 96, 174, 174]\n",
    "\n",
    "vowel_list = ['i', 'L', 'e', 'F', 'u', 'W', 'o', 'D']\n",
    "vowel_formants = [\n",
    "    [3372, 2761, 437],\n",
    "    [3053, 2365, 483],\n",
    "    [3047, 2530, 536],\n",
    "    [2979, 2058, 731],\n",
    "    [2735, 1105, 459],\n",
    "    [2827, 1225, 519],\n",
    "    [2828, 1035, 555],\n",
    "    [2824, 1136, 781]\n",
    "]\n",
    "\n",
    "# no. of tokens for each word\n",
    "sample_size = 8000\n",
    "\n",
    "metadata = []\n",
    "vcv_list = []\n",
    "for index in range (4):\n",
    "    for jndex in range (8):\n",
    "\n",
    "        consonant = consonant_list[index]\n",
    "        cog = consonant_cog[index]\n",
    "        fri_dur = consonant_fd[index]\n",
    "\n",
    "        vowel = vowel_list[jndex]\n",
    "        f_vals = vowel_formants[jndex]\n",
    "\n",
    "        word = vowel + consonant + vowel\n",
    "\n",
    "        if (index in [0, 3]) and (jndex in [0, 2, 4, 6]):\n",
    "            istrain = 'yes'\n",
    "        else:\n",
    "            istrain = 'no'\n",
    "\n",
    "        c_f_means = np.array([cog, fri_dur])\n",
    "        c_f_stds = np.array([500, 13])\n",
    "\n",
    "        con_means = np.array([200, 0.5, 1, 50, 60]) # sta_dev, skewness, kurtosis, bur_int, fri_int\n",
    "        con_stds = con_means * 0.05\n",
    "\n",
    "        con_means = np.concatenate((c_f_means, con_means))\n",
    "        con_stds = np.concatenate((c_f_stds, con_stds))\n",
    "\n",
    "        consonants = np.zeros((sample_size, len(con_means)))\n",
    "        for i in range(len(con_means)):\n",
    "            a, b = - 2*con_stds[i] / con_stds[i], 2*con_stds[i] / con_stds[i]\n",
    "            dist = truncnorm(a, b, loc = con_means[i], scale = con_stds[i])\n",
    "            consonants[:, i] = dist.rvs(size = sample_size)\n",
    "        \n",
    "        # total duration of fixed value 200 for consonants\n",
    "        con_dur = np.full((sample_size, 1), 200)\n",
    "\n",
    "        # zero values for all other features\n",
    "        zeros = np.full((sample_size, 9), 0)\n",
    "\n",
    "        # all concatenated\n",
    "        consonants = np.hstack((consonants, con_dur, zeros))\n",
    "\n",
    "        vow_means = np.array([80, 200, 170, 110 ,90]) # voc_int, f0, b3, b2, b1\n",
    "        vow_means = np.append(vow_means, f_vals) # *, f3, f2, f1\n",
    "        vow_stds = vow_means * 0.05\n",
    "\n",
    "        vowels = np.zeros((sample_size, len(vow_means)))\n",
    "        for i in range(len(vow_means)):\n",
    "            a, b = - 2*vow_stds[i] / vow_stds[i], 2*vow_stds[i] / vow_stds[i]\n",
    "            dist = truncnorm(a, b, loc = vow_means[i], scale = vow_stds[i])\n",
    "            vowels[:, i] = dist.rvs(size = sample_size)\n",
    "\n",
    "        # vocalic duration and total duration of fixed value 400 for vowels\n",
    "        vow_dur = np.full((sample_size, 2), 400)\n",
    "\n",
    "        # zero values for all other features\n",
    "        zeros = np.full((sample_size, 7), 0)\n",
    "\n",
    "        # all concatenated\n",
    "        vowels = np.hstack((zeros, vow_dur, vowels))\n",
    "\n",
    "        subdata_dir = os.path.join(data_dir, word)\n",
    "        os.makedirs(subdata_dir, exist_ok=True)\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            uid = word + f'_{i+1:04d}'\n",
    "            filename = f'{uid}.npy'\n",
    "            # save_path = os.path.join(subdata_dir, filename)\n",
    "            \n",
    "            vcv = np.vstack([vowels[i], consonants[i], vowels[i]])\n",
    "            outer_idx = len(vcv_list)\n",
    "            vcv_list.append(vcv)\n",
    "\n",
    "            # np.save(save_path, vcv)\n",
    "            \n",
    "            # save_path_rel = os.path.relpath(save_path, start=work_dir)\n",
    "            save_path_rel = outer_idx\n",
    "\n",
    "            metadata.append({\n",
    "                'uid': uid,\n",
    "                'path': save_path_rel,\n",
    "                'cog': vcv[1][0],\n",
    "                'fri_dur': vcv[1][1],\n",
    "                'word': word,\n",
    "                'consonant': consonant,\n",
    "                'vowel': vowel,\n",
    "                'train': istrain\n",
    "            })\n",
    "\n",
    "csv_name = 'metadata.csv'\n",
    "csv_path = os.path.join(data_dir, csv_name)\n",
    "metaframe = pd.DataFrame(metadata)\n",
    "metaframe.to_csv(csv_path, index=False)\n",
    "\n",
    "vcv_all = np.stack(vcv_list, axis=0)   # shape (sample_size, 3, 17)\n",
    "save_path = os.path.join(data_dir, \"all_data.npy\")\n",
    "np.save(save_path, vcv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d771859-ec80-4bbf-826c-fdc07143b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(os.path.join(data_dir, \"all_data.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e88c8c93-40db-4495-a55a-bdfb4b096c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256000, 3, 17)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
